{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('pima_redux.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connection:\n",
    "    def __init__(self, connectedNeuron, weight=None):\n",
    "        self.connectedNeuron = connectedNeuron\n",
    "        \n",
    "        if weight:\n",
    "            self.weight = weight\n",
    "        else:\n",
    "            self.weight = np.random.normal()\n",
    "            \n",
    "        self.dWeight = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    alpha = 1\n",
    "\n",
    "    def __init__(self, previousLayer, isOutput=False, weights=None):\n",
    "        self.connections = [] #connections\n",
    "        self.error = 0.0\n",
    "        self.gradient = 0.0\n",
    "        self.z = 0.0\n",
    "        self.value = 0.0\n",
    "        self.isOutput = isOutput\n",
    "        if previousLayer:\n",
    "            #conecta o neuronio com a camada anterior\n",
    "            for neuron, index in zip(previousLayer, range(len(previousLayer))):\n",
    "                if weights:\n",
    "                    connection = Connection(neuron, weights[index])\n",
    "                else:\n",
    "                    connection = Connection(neuron)\n",
    "                self.connections.append(connection)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def addError(self, err):\n",
    "        self.error = self.error + err\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x * 1.0))\n",
    "\n",
    "    def dSigmoid(self, x):\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "    def setError(self, err):\n",
    "        self.error = err\n",
    "\n",
    "    def setValue(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def getValue(self):\n",
    "        return self.value\n",
    "    \n",
    "    def feedForward(self):\n",
    "        sumValue = 0\n",
    "        if len(self.connections) == 0:\n",
    "            print('a', self.value)\n",
    "            return\n",
    "        for connection in self.connections:\n",
    "            sumValue = sumValue + connection.connectedNeuron.getValue() * connection.weight\n",
    "        print('z:', round(sumValue,5))\n",
    "        self.value = self.sigmoid(sumValue)\n",
    "        print('a:', round(self.value,5))\n",
    "        \n",
    "    # δweight= η x gradient x output of connected neuron + α x previous δweight\n",
    "    # trabalho atualiza de outra forma\n",
    "    \n",
    "    def backPropagate(self):\n",
    "#         pdb.set_trace()\n",
    "        #propagates it's error to the previous layer neurons\n",
    "        if self.connections:\n",
    "            for connection in self.connections:\n",
    "                if connection.connectedNeuron.connections:                    \n",
    "                    connection.connectedNeuron.error += self.error * connection.weight\n",
    "\n",
    "            #calculate it's own error\n",
    "            if not self.isOutput:\n",
    "                self.error = self.error * self.value * (1 - self.value)\n",
    "            print('Delta: ', round(self.error,5))\n",
    "#             pdb.set_trace()\n",
    "            for connection in self.connections:\n",
    "                self.gradient = self.error * connection.connectedNeuron.value\n",
    "                print('Gradient: ', round(self.gradient,5))\n",
    "                connection.weight = connection.weight - (self.alpha * self.gradient)\n",
    "#                 connection.weight = self.gradient * regularization\n",
    "#                 print('Delta: ', round(self.error,5))\n",
    "\n",
    "            self.error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, topology, weights=None, regularization=1):\n",
    "        self.layers = []\n",
    "        self.regularization = regularization\n",
    "        self.j = 0.0\n",
    "        \n",
    "        for numNeuron, index in zip(topology, range(len(topology))):\n",
    "            layer = []\n",
    "            layer.append(Neuron(None)) # bias neuron\n",
    "            layer[-1].setValue(1) # setting output of bias neuron as 1\n",
    "            if index == len(topology) -1:\n",
    "                isOutput = True\n",
    "            else:\n",
    "                isOutput = False\n",
    "            #\n",
    "            for i in range(numNeuron):\n",
    "                \n",
    "                #input layer\n",
    "                if (len(self.layers) == 0):\n",
    "                    layer.append(Neuron(None))\n",
    "                else:\n",
    "                    #recebe layer de trás\n",
    "                    if weights:\n",
    "                        layerWeights = weights[index-1]\n",
    "                        layer.append(Neuron(self.layers[-1], isOutput, layerWeights[i]))\n",
    "                    else:\n",
    "                        layer.append(Neuron(self.layers[-1], isOutput))\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "\n",
    "            \n",
    "            \n",
    "    def setInput(self, inputs):\n",
    "        for i in range(len(inputs)):\n",
    "            self.layers[0][i+1].setValue(inputs[i])\n",
    "              \n",
    "\n",
    "    def getError(self, target):\n",
    "        error = 0\n",
    "        \n",
    "        if (type(target) != list):\n",
    "            \n",
    "            e = (target - self.layers[-1][1].getValue())\n",
    "            error = error + e ** 2\n",
    "        \n",
    "        else:\n",
    "            for i in range(len(target)):\n",
    "                e = (target[i] - self.layers[-1][i].getValue())\n",
    "                error = error + e ** 2\n",
    "            \n",
    "            error = error / len(target)\n",
    "            \n",
    "        error = math.sqrt(error)\n",
    "        return error\n",
    "        \n",
    "    def jFunction(self, output, target):\n",
    "        if (type(target) != list):\n",
    "#             output = self.layers[-1][1].getValue()\n",
    "            return - (target) * (np.log(output)) - (1 - target) * (np.log(1 - output))\n",
    "\n",
    "        else:\n",
    "            j = 0.0\n",
    "            for i in range(len(target)):\n",
    "#                 output = self.layers[-1][i+1].getValue()\n",
    "                #j  +=  (target[i] * -1) * (np.log(output)) - (1 - target[i]) * (np.log(1 - output)) + (self.regularization / (2 * inputSize)) + self.squaredWeightsSum()\n",
    "                j  +=  (target[i] * -1) * (np.log(output)) - (1 - target[i]) * (np.log(1 - output))\n",
    "            return j\n",
    "        \n",
    "    \n",
    "    def feedForward(self):\n",
    "#         pdb.set_trace()\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer:\n",
    "                neuron.feedForward();\n",
    "                \n",
    "#     def backPropagate(self, target):\n",
    "#         if (type(target) != list):\n",
    "#             self.layers[-1][1].setError(self.layers[-1][1].getValue() - target)\n",
    "#         else:\n",
    "#             for i in range(len(target)):\n",
    "#                 self.layers[-1][i+1].setError(self.layers[-1][i+1].getValue() - target[i])\n",
    "            \n",
    "#         for layer in self.layers[::-1]: #reverse the order\n",
    "#             for neuron in layer:\n",
    "#                 neuron.backPropagate(self.regularization)\n",
    "    \n",
    "    def backPropagate(self, target):\n",
    "        \n",
    "        if (type(target) != list):\n",
    "#             pdb.set_trace()\n",
    "            self.layers[-1][1].setError(self.layers[-1][1].getValue() - target)\n",
    "            print('f(x): ',round(self.layers[-1][1].getValue(),5))\n",
    "            print('target: ', target)\n",
    "            j = self.jFunction(self.layers[-1][1].getValue(), target)\n",
    "            print('j: ', round(j,3))\n",
    "        else:\n",
    "            j = 0.0\n",
    "            for i in range(len(target)):\n",
    "                # y - ftheta\n",
    "#                 pdb.set_trace()\n",
    "                self.layers[-1][i+1].setError(self.layers[-1][i+1].getValue() - target[i])\n",
    "                j += self.jFunction(self.layers[-1][i+1].getValue(), target)\n",
    "                print('f(x): ',round(self.layers[-1][i+1].getValue(),5))\n",
    "                print('target: ', target[i])\n",
    "            print('j: ', round((j/len(target)),3))\n",
    "            \n",
    "            \n",
    "        for layer in self.layers[::-1]: #reverse the order\n",
    "            for neuron in layer:\n",
    "                neuron.backPropagate()\n",
    "                \n",
    "    def getResults(self):\n",
    "        output = []\n",
    "        for neuron in self.layers[-1]:\n",
    "            \n",
    "            #ignore bias neuron\n",
    "            if not neuron.connections:\n",
    "                continue\n",
    "            output.append(neuron.getValue())\n",
    "        return output\n",
    "\n",
    "    def getThResults(self):\n",
    "        output = []\n",
    "        for neuron in self.layers[-1]:\n",
    "            \n",
    "            # ignore bias neuron\n",
    "            if not neuron.connections:\n",
    "                continue\n",
    "            \n",
    "            o = neuron.getValue()\n",
    "            if (o > 0.5):\n",
    "                o = 1\n",
    "            else:\n",
    "                o = 0\n",
    "            output.append(o)\n",
    "        #output.pop()# removing the bias neuron\n",
    "        return output\n",
    "    \n",
    "    def train(self, inputs, outputs):\n",
    "         \n",
    "        error = 0\n",
    "    \n",
    "        for i in range(len(inputs)):\n",
    "            self.setInput(inputs[i])\n",
    "            self.feedForward()\n",
    "\n",
    "            self.backPropagate(outputs[i])\n",
    "            error = error + self.getError(outputs[i])\n",
    "#             print (\"error: \", error)\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "        return (data - min(data))/ (max(data) - min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetColumn = 'target'\n",
    "# dfWithoutTarget = df.loc[:, df.columns != targetColumn]\n",
    "# outputs = df[targetColumn].values\n",
    "\n",
    "# for feature in dfWithoutTarget.columns:\n",
    "#     df[feature] = normalize(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = df.loc[:, df.columns != targetColumn].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60181\n"
     ]
    }
   ],
   "source": [
    "x = round(0.6018070039467833, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "a 0.83\n",
      "a 0.02\n",
      "a 1\n",
      "z: 0.5525\n",
      "a: 0.63472\n",
      "z: 0.8138\n",
      "a: 0.69292\n",
      "z: 0.1761\n",
      "a: 0.54391\n",
      "z: 0.6041\n",
      "a: 0.64659\n",
      "a 1\n",
      "z: 1.81696\n",
      "a: 0.8602\n",
      "z: 2.02468\n",
      "a: 0.88336\n",
      "z: 1.37327\n",
      "a: 0.79791\n",
      "a 1\n",
      "z: 1.58228\n",
      "a: 0.82953\n",
      "z: 1.64577\n",
      "a: 0.83832\n",
      "f(x):  0.82953\n",
      "target:  0.75\n",
      "f(x):  0.83832\n",
      "target:  0.28\n",
      "j:  1.929\n",
      "Delta:  0.07953\n",
      "Gradient:  0.07953\n",
      "Gradient:  0.06841\n",
      "Gradient:  0.07025\n",
      "Gradient:  0.06346\n",
      "Delta:  0.55832\n",
      "Gradient:  0.55832\n",
      "Gradient:  0.48027\n",
      "Gradient:  0.4932\n",
      "Gradient:  0.44549\n",
      "Delta:  0.01503\n",
      "Gradient:  0.01503\n",
      "Gradient:  0.00954\n",
      "Gradient:  0.01042\n",
      "Gradient:  0.00818\n",
      "Gradient:  0.00972\n",
      "Delta:  0.05809\n",
      "Gradient:  0.05809\n",
      "Gradient:  0.03687\n",
      "Gradient:  0.04025\n",
      "Gradient:  0.0316\n",
      "Gradient:  0.03756\n",
      "Delta:  0.06892\n",
      "Gradient:  0.06892\n",
      "Gradient:  0.04374\n",
      "Gradient:  0.04775\n",
      "Gradient:  0.03748\n",
      "Gradient:  0.04456\n",
      "Delta:  0.12981\n",
      "Gradient:  0.12981\n",
      "Gradient:  0.10775\n",
      "Gradient:  0.0026\n",
      "Delta:  0.10047\n",
      "Gradient:  0.10047\n",
      "Gradient:  0.08339\n",
      "Gradient:  0.00201\n",
      "Delta:  0.14769\n",
      "Gradient:  0.14769\n",
      "Gradient:  0.12258\n",
      "Gradient:  0.00295\n",
      "Delta:  0.14831\n",
      "Gradient:  0.14831\n",
      "Gradient:  0.1231\n",
      "Gradient:  0.00297\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    topology = [2,4,3,2]\n",
    "#     topology.append(8)\n",
    "#     topology.append(8)\n",
    "#     topology.append(1)\n",
    "\n",
    "\n",
    "#     weights = [[[0.4, 0.1], [0.3, 0.2]], [[0.7, 0.5, 0.6]]]\n",
    "    weights = [\n",
    "        [[0.42000, 0.15000, 0.40000], [0.72000, 0.10000, 0.54000 ], [0.01000, 0.19000, 0.42000], [0.30000, 0.35000, 0.68000 ]], \n",
    "        [[0.21000, 0.67000, 0.14000, 0.96000, 0.87000],[0.87000, 0.42000, 0.20000, 0.32000, 0.89000],[0.03000, 0.56000, 0.80000, 0.69000, 0.09000]],\n",
    "        [[0.04000, 0.87000, 0.42000, 0.53000],[0.17000, 0.10000, 0.95000, 0.69000]]\n",
    "    ]\n",
    "    net = Network(topology, weights)\n",
    "    Neuron.eta = 0.09\n",
    "    Neuron.alpha = 0.015\n",
    "    \n",
    "    inputs = [[0.83000, 0.02000]]\n",
    "    outputs = [[0.75000, 0.28000]]\n",
    "    net.train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"end\")\n",
    "            \n",
    "#     while True:\n",
    "#         a = input(\"type 1st input :\")\n",
    "#         b = input(\"type 2nd input :\")\n",
    "#         net.setInput([a, b])\n",
    "#         net.feedForward()\n",
    "#         print (net.getThResults()) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
